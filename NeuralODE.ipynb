{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Ordinary Differential Equations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "На конференции NIPS 2019 года была представлена статья,\n",
    "которая называется 'Нейронные обычные дифференциальные уравнения'.\n",
    "Эта статья была в числе 4 из 4854 статей, которая получила награду\n",
    "'Best paper'. В своей работе я постараюсь разобрать метод, предлагаемый\n",
    "авторами и сравнить данную архитектуру с уже существующими решениями"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Вступление"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Что происходит в обычных нейронных сетях? В них есть слои(скрытые состояния) $ h_t $ и переходы между этими состояниями\n",
    " происходят с помощью умножений на соответствующие матрицы весов $ W_t $.\n",
    "<img src=assets/mlp.png width=600 ></img>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "У остаточных нейронных сетей принцип действия такой же, за исключением того, что на выходе из слоя добавляется\n",
    "еще и исходное состояние: <img src=assets/res_learn.png width=600 ></img>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Идея этого заключается в том, что мы не очень хотим чтобы наше состояние кардинально отличалось от\n",
    "предыдущего на каждом шаге.\n",
    "\n",
    "Состояния остаточных нейронных сетей описываются следующей формулой: $h_{t+1} = h_{t} + f(h_{t}, \\theta_{t})$, где $t \\in \\{0...T\\}$ -\n",
    "это номер блока, $ \\theta_t $ - матрица весов, а $f $ - функция активации соответствующего слоя."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "А что если $ T $ устремить к бесконечности, а $ t_{i} - t_{i-1} $ , то есть, шаг, устремить к нулю?\n",
    "Тогда это можно рассматривать как некий временной процесс.\n",
    "Следовательно, это можно записать в виде дифференциального уравнения: $\\frac{dh(t)}{dt} = f(h(t), t, \\theta)$\n",
    "\n",
    "Тогда, решая это дифференциальное уравнение можно восстановить искомую зависимость.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Как решать такое дифференциальное уравнение?\n",
    "В исходной статье используется метод Эйлера. Как он работает: <img src=assets/euler.jpg width=600 ></img>\n",
    "Численное решение задается формулой $ y_i = y_{i -1} + hf(x_{i-1}, y_{i -1}) $\n",
    "\n",
    "То есть, на каждой итерации мы делаем маленький шаг в сторону градиента функции. В конечном итоге\n",
    "после всех итераций мы восстановим искомую функцию."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обучение такой нейронной сети заключается в том, что нам необходимо найти такие веса $ \\theta $ , чтобы\n",
    "получить правильные градиенты $\\frac{dh(t)}{dt} $ такие, что когда мы решаем ОДУ, мы получим правильную функцию\n",
    ", которая в конечный момент времени $ t $ , то есть, на выходном\n",
    "слое выдаст правильный ответ с желаемой точностью."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Как мы будем обучать такую нейросеть?\n",
    "\n",
    "Наша нейросеть определяет переходы от одного состояния к следующему. У нее есть параметры $ \\theta $ , которые\n",
    "нам и нужно обучить. А обучать мы их будем с помощью обычного градиентного спуска, прямо как в\n",
    "классической нейронной сети. Но не все так просто.\n",
    "\n",
    "<img src=assets/loss.png width=600 ></img>\n",
    "\n",
    "Функция потери в свою очередь задается следующим образом:\n",
    "$ L(z(t_1)) = L(z(t_0) + \\int_{t_0}^{t_1} f(z(t), t, \\theta) dt) = L(ODESolve(z(t_0), f, t_0, t_1, \\theta))  $\n",
    "\n",
    "Таким образом, чтобы оптимизировать $ L $ нам нужны произодные $ \\frac{dL}{d\\theta} $ .\n",
    "Чтобы это сделать нужно определить как функция потерь зависит от $ z(t) $ в каждый момент.\n",
    "Эта зависимость обозначается как $ a(t) = \\frac{dL}{dz(t)} $ и называется *сопряженным* состоянием.\n",
    "\n",
    "Тогда получим: $ \\frac{d a(t)}{d t} = -a(t) \\frac{\\partial f(z(t), t, \\theta)}{\\partial z}\n",
    " $ (вывод этой формулы можно посмотреть в оригинальной статье).\n",
    "\n",
    "Решая этот дифур можно получить все необходимые частные производные:\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial z(t_0)} = \\int_{t_1}^{t_0} a(t) \\frac{\\partial f(z(t), t, \\theta)}{\\partial z} dt\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial \\theta} = \\int_{t_1}^{t_0} a(t) \\frac{\\partial f(z(t), t, \\theta)}{\\partial \\theta} dt\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial t_0} = \\int_{t_1}^{t_0} a(t) \\frac{\\partial f(z(t), t, \\theta)}{\\partial t} dt\n",
    "$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Все эти частные производные могут быть посчитаны за один вызов ODESolve:\n",
    "<img src=assets/algo.png width=600 ></img>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Практика\n",
    "\n",
    "Авторы статьи говорят, что такая архитектура требует константной памяти, адаптирует стратегию вычисления\n",
    "к каждым входным данным и позволяет выгодно обменивать численную точность на скорость\n",
    "\n",
    "Это звучит очень круто, но так ли оно на самом деле?\n",
    "\n",
    "Посмотрим как оно умеет решать диффуры."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/rtqichen/torchdiffeq.git"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!cd torchdiffeq && pip install -e ."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# from torchdiffeq.torchdiffeq import odeint\n",
    "from torchdiffeq import odeint\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_time = 10\n",
    "batch_size = 20\n",
    "data_size = 200\n",
    "\n",
    "viz = True\n",
    "\n",
    "def get_batch():\n",
    "    s = torch.from_numpy(np.random.choice(np.arange(data_size - batch_time, dtype=np.int64), batch_size, replace=False))\n",
    "    batch_y0 = true_y[s]  # (M, D)\n",
    "    batch_t = t[:batch_time]  # (T)\n",
    "    batch_y = torch.stack([true_y[s + i] for i in range(batch_time)], dim=0)  # (T, M, D)\n",
    "    return batch_y0, batch_t, batch_y\n",
    "\n",
    "\n",
    "def makedirs(dirname):\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "\n",
    "DATA_TYPE = 'NNET_MORE'\n",
    "\n",
    "if viz:\n",
    "    makedirs(DATA_TYPE)\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize(true_y, pred_y, odefunc, itr):\n",
    "\n",
    "    if viz:\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(t.numpy(), true_y.numpy()[:, 0, 0], t.numpy(), true_y.numpy()[:, 0, 1], 'g-')\n",
    "        plt.plot(t.numpy(), pred_y.numpy()[:, 0, 0], '--', t.numpy(), pred_y.numpy()[:, 0, 1], 'b--')\n",
    "        plt.savefig(DATA_TYPE + '/ts' + str(itr) + '.png')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(true_y.numpy()[:, 0, 0], true_y.numpy()[:, 0, 1], 'g-')\n",
    "        plt.plot(pred_y.numpy()[:, 0, 0], pred_y.numpy()[:, 0, 1], 'b--')\n",
    "        plt.savefig(DATA_TYPE + '/phase' + str(itr) + '.png')\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RunningAverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, momentum=0.99):\n",
    "        self.momentum = momentum\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = None\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        if self.val is None:\n",
    "            self.avg = val\n",
    "        else:\n",
    "            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n",
    "        self.val = val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Зададим систему дифференциальных уравнений следующего вида:\n",
    "$Y' = AY$ где $A = \\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d \\end{bmatrix}$ и $Y$ = $\\begin{bmatrix} y_1 (t) \\\\ y_2 (t)\n",
    "\\end{bmatrix}$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Пусть $A = \\begin{bmatrix}\n",
    "0 & -2 \\\\\n",
    "2 & 0\n",
    "\\end{bmatrix}$\n",
    "и $Y(0) = \\begin{bmatrix}\n",
    "2 \\\\\n",
    "0\n",
    "\\end{bmatrix}$\n",
    "Тогда решая систему получим\n",
    "$ Y = \\begin{bmatrix}\n",
    "2\\cos(t) \\\\\n",
    "2\\sin(t)\n",
    "\\end{bmatrix} $, оно же является параметрическим уравнением эллипса"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "true_y0 = torch.tensor([[2., 0.]])\n",
    "t = torch.linspace(0., 25., 1000)\n",
    "true_A = torch.tensor([[0, -2.], [2., 0]])\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        return torch.mm(y, true_A)\n",
    "\n",
    "with torch.no_grad():\n",
    "    true_y = odeint(Lambda(), true_y0, t, method='dopri5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "На самом деле, метод Эйлера решения дифференциальных уравнений сейчас считается\n",
    "неточным и уже устаревшим.\n",
    "Поэтому, внутри функции *odeint* реализованы более\n",
    "сложные методы решения. Сейчас выведем на плоскость решение этого диффура.\n",
    "Напомню, что должен получиться эллипс."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ellips = plt.figure()\n",
    "plt.plot(true_y.numpy()[:, 0, 0], true_y.numpy()[:, 0, 1], 'g-')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "ellips.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Да, как видим, так и вышло.\n",
    "\n",
    "Далее создадим модель, которую будем обучать. Она будет совсем простой."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ODEFunc(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ODEFunc, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 150),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(150, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 2),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        return self.net(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ii = 0\n",
    "niters = 5000\n",
    "\n",
    "func = ODEFunc()\n",
    "optimizer = optim.RMSprop(func.parameters(), lr=1e-3)\n",
    "end = time.time()\n",
    "\n",
    "time_meter = RunningAverageMeter(0.97)\n",
    "loss_meter = RunningAverageMeter(0.97)\n",
    "\n",
    "for itr in range(1, niters + 1):\n",
    "    optimizer.zero_grad()\n",
    "    batch_y0, batch_t, batch_y = get_batch()\n",
    "    pred_y = odeint(func, batch_y0, batch_t)\n",
    "    loss = torch.mean(torch.abs(pred_y - batch_y))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    time_meter.update(time.time() - end)\n",
    "    loss_meter.update(loss.item())\n",
    "\n",
    "    if itr % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            pred_y = odeint(func, true_y0, t)\n",
    "            loss = torch.mean(torch.abs(pred_y - true_y))\n",
    "            print('Iter {:04d} | Total Loss {:.6f}'.format(itr, loss.item()))\n",
    "            visualize(true_y, pred_y, func, ii)\n",
    "            ii += 1\n",
    "\n",
    "    end = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Источники"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Исходная статья - https://arxiv.org/abs/1806.07366\n",
    "\n",
    "Авторский репозиторий - https://github.com/rtqichen/torchdiffeq\n",
    "\n",
    "Имплементация ODENet в tensorflow - https://github.com/jason71995/Keras_ODENet\n",
    "\n",
    "Имплементация ResNet в tensorflow - https://github.com/SeHwanJoo/cifar10-ResNet-tensorflow\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}